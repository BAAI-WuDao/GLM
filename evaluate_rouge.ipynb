{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cheap-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/zhengxiao/Downloads/generation-large-cnndm/test.jsonl.hyps\") as file:\n",
    "    hyps = file.readlines()\n",
    "    hyps = [line.strip() for line in hyps]\n",
    "\n",
    "with open(\"/Users/zhengxiao/Downloads/generation-large-cnndm/test.jsonl.refs\") as file:\n",
    "    refs = file.readlines()\n",
    "    refs = [line.strip() for line in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "together-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def _is_digit(w):\n",
    "    for ch in w:\n",
    "        if not (ch.isdigit() or ch == ','):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def fix_tokenization(text):\n",
    "    input_tokens = text.split()\n",
    "    output_tokens = []\n",
    "    has_left_quote = False\n",
    "    has_left_single_quote = False\n",
    "\n",
    "    i = 0\n",
    "    prev_dash = False\n",
    "    while i < len(input_tokens):\n",
    "        tok = input_tokens[i]\n",
    "        flag_prev_dash = False\n",
    "        if tok == \"\\\"\":\n",
    "            if has_left_quote:\n",
    "                output_tokens.append(\"''\")\n",
    "            else:\n",
    "                output_tokens.append(\"``\")\n",
    "            has_left_quote = not has_left_quote\n",
    "            i += 1\n",
    "        elif tok == \"'\" and len(output_tokens) > 0 and output_tokens[-1].endswith(\"n\") and i < len(input_tokens) - 1 and \\\n",
    "                input_tokens[i + 1] == \"t\":\n",
    "            output_tokens[-1] = output_tokens[-1][:-1]\n",
    "            output_tokens.append(\"n't\")\n",
    "            i += 2\n",
    "        elif tok == \"'\" and i < len(input_tokens) - 1 and input_tokens[i + 1] in (\"s\", \"d\", \"ll\"):\n",
    "            output_tokens.append(\"'\" + input_tokens[i + 1])\n",
    "            i += 2\n",
    "        elif tok == \"'\":\n",
    "            if has_left_single_quote:\n",
    "                output_tokens.append(\"'\")\n",
    "            else:\n",
    "                output_tokens.append(\"`\")\n",
    "            has_left_single_quote = not has_left_single_quote\n",
    "            i += 1\n",
    "        elif tok == \".\" and i < len(input_tokens) - 2 and input_tokens[i + 1] == \".\" and input_tokens[i + 2] == \".\":\n",
    "            output_tokens.append(\"...\")\n",
    "            i += 3\n",
    "        elif tok == \",\" and len(output_tokens) > 0 and _is_digit(output_tokens[-1]) and i < len(\n",
    "                input_tokens) - 1 and _is_digit(input_tokens[i + 1]):\n",
    "            # $ 3 , 000 -> $ 3,000\n",
    "            output_tokens[-1] += ',' + input_tokens[i + 1]\n",
    "            i += 2\n",
    "        elif tok == \".\" and len(output_tokens) > 0 and output_tokens[-1].isdigit() and i < len(input_tokens) - 1 and \\\n",
    "                input_tokens[i + 1].isdigit():\n",
    "            # 3 . 03 -> $ 3.03\n",
    "            output_tokens[-1] += '.' + input_tokens[i + 1]\n",
    "            i += 2\n",
    "        elif tok == \".\" and len(output_tokens) > 0 and len(output_tokens[-1]) == 1 and output_tokens[\n",
    "            -1].isupper() and i < len(input_tokens) - 2 and len(input_tokens[i + 1]) == 1 and input_tokens[\n",
    "            i + 1].isupper() and input_tokens[i + 2] == '.':\n",
    "            # U . N . -> U.N.\n",
    "            k = i + 3\n",
    "            while k + 2 < len(input_tokens):\n",
    "                if len(input_tokens[k + 1]) == 1 and input_tokens[k + 1].isupper() and input_tokens[k + 2] == '.':\n",
    "                    k += 2\n",
    "                else:\n",
    "                    break\n",
    "            output_tokens[-1] += ''.join(input_tokens[i:k])\n",
    "            i += 2\n",
    "        elif tok == \"-\":\n",
    "            if i < len(input_tokens) - 1 and input_tokens[i + 1] == \"-\":\n",
    "                output_tokens.append(\"--\")\n",
    "                i += 2\n",
    "            elif i == len(input_tokens) - 1 or i == 0:\n",
    "                output_tokens.append(\"-\")\n",
    "                i += 1\n",
    "            elif output_tokens[-1] not in string.punctuation and input_tokens[i + 1][0] not in string.punctuation:\n",
    "                output_tokens[-1] += \"-\"\n",
    "                i += 1\n",
    "                flag_prev_dash = True\n",
    "            else:\n",
    "                output_tokens.append(\"-\")\n",
    "                i += 1\n",
    "        elif prev_dash and len(output_tokens) > 0 and tok[0] not in string.punctuation:\n",
    "            output_tokens[-1] += tok\n",
    "            i += 1\n",
    "        else:\n",
    "            output_tokens.append(tok)\n",
    "            i += 1\n",
    "        prev_dash = flag_prev_dash\n",
    "    return \" \".join(output_tokens)\n",
    "\n",
    "\n",
    "def count_tokens(tokens):\n",
    "    counter = {}\n",
    "    for t in tokens:\n",
    "        if t in counter.keys():\n",
    "            counter[t] += 1\n",
    "        else:\n",
    "            counter[t] = 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def get_f1(text_a, text_b):\n",
    "    tokens_a = text_a.lower().split()\n",
    "    tokens_b = text_b.lower().split()\n",
    "    if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "        return 1 if len(tokens_a) == len(tokens_b) else 0\n",
    "    set_a = count_tokens(tokens_a)\n",
    "    set_b = count_tokens(tokens_b)\n",
    "    match = 0\n",
    "    for token in set_a.keys():\n",
    "        if token in set_b.keys():\n",
    "            match += min(set_a[token], set_b[token])\n",
    "    p = match / len(tokens_a)\n",
    "    r = match / len(tokens_b)\n",
    "    return 2.0 * p * r / (p + r + 1e-5)\n",
    "\n",
    "\n",
    "def remove_duplicate(l_list, duplicate_rate):\n",
    "    tk_list = [l.lower().split() for l in l_list]\n",
    "    r_list = []\n",
    "    history_set = set()\n",
    "    for i, w_list in enumerate(tk_list):\n",
    "        w_set = set(w_list)\n",
    "        if len(w_set & history_set) / len(w_set) <= duplicate_rate:\n",
    "            r_list.append(l_list[i])\n",
    "        history_set |= w_set\n",
    "    return r_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "logical-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42271185970285236\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "metric = \"rouge-1\"\n",
    "duplicate_rate = 0.7\n",
    "metric_dict = {\"rouge-1\": \"rouge1\", \"rouge-2\": \"rouge2\", \"rouge-l\": \"rougeLsum\"}\n",
    "ref_list = []\n",
    "for ref in refs:\n",
    "    ref = ref.strip().split('[SEP]')\n",
    "    ref = [fix_tokenization(sentence) for sentence in ref]\n",
    "    ref = \"\\n\".join(ref)\n",
    "    ref_list.append(ref)\n",
    "pred_list = []\n",
    "for prediction in hyps:\n",
    "    buf = []\n",
    "    for sentence in prediction.strip().split(\"[SEP]\"):\n",
    "        sentence = fix_tokenization(sentence)\n",
    "        if any(get_f1(sentence, s) > 1.0 for s in buf):\n",
    "            continue\n",
    "        s_len = len(sentence.split())\n",
    "        if s_len <= 4:\n",
    "            continue\n",
    "        buf.append(sentence)\n",
    "    if duplicate_rate and duplicate_rate < 1:\n",
    "        buf = remove_duplicate(buf, duplicate_rate)\n",
    "    line = \"\\n\".join(buf)\n",
    "    pred_list.append(line)\n",
    "scorer = rouge_scorer.RougeScorer([metric_dict[metric]], use_stemmer=True)\n",
    "scores = [scorer.score(pred, ref) for pred, ref in zip(pred_list, ref_list)]\n",
    "scores = [score[metric_dict[metric]].fmeasure for score in scores]\n",
    "scores = sum(scores) / len(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "devoted-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the raiders stripped off before entering the esso station in caterham .\n",
      "police have released cctv images of two of the three would-be raiders .\n",
      "the men escaped with one staff member 's mobile phone but no cash .\n",
      "a third man has been arrested and is on police bail following the incident .\n",
      "\n",
      "police in surrey have released cctv of two raiders who stripped-off their shirts before attempting to rob an esso petrol station .\n",
      "the men , who were both in their 20s , forced their way into the manager 's office of the filling station in town end , caterham , surrey , looking for cash .\n",
      "they were disturbed by a member of the garage staff and fled .\n",
      "a third man , aged in his 20s , had been arrested and released on police bail while the investigation continues .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randrange(len(ref_list))\n",
    "print(ref_list[idx])\n",
    "print(\"\")\n",
    "print(pred_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "future-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.tokenization import make_tokenizer\n",
    "tokenizer = make_tokenizer(\"BertWordPieceTokenizer\", None, None, None, \"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controversial-printer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.EncodeAsIds(\"[MASK]\").tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "posted-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the student was identified during an investigation by campus police and the office of student affairs . officials are still trying to determine if other people were involved . students march and chant , `` we are not afraid . we stand together , '' after pictures of the noose are passed around on social media .\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_tokenization(hyps[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "exotic-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [tokenizer.DecodeIds(tokenizer.EncodeAsIds(ref).tokenization) for ref in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "plain-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = [fix_tokenization(hyp) for hyp in hyps]\n",
    "refs = [fix_tokenization(ref) for ref in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "veterinary-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = [hyp.replace(\"< unk >\", \"unk\") for hyp in hyps]\n",
    "refs = [ref.replace(\"< unk >\", \"unk\") for ref in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "systematic-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./generation-large-ggw_test/test.jsonl.hyps_new\", \"w\") as output:\n",
    "    for hyp in hyps:\n",
    "        output.write(hyp + \"\\n\")\n",
    "with open(\"./generation-large-ggw_test/test.jsonl.refs_new\", \"w\") as output:\n",
    "    for ref in refs:\n",
    "        output.write(ref + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "desirable-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line_break(text):\n",
    "    text = text.split(\" . \")\n",
    "    text = \" . \\n \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "photographic-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = [add_line_break(hyp) for hyp in hyps]\n",
    "refs = [add_line_break(ref) for ref in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "careful-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.41868113530622203, 'p': 0.39858987750197444, 'r': 0.46918404550355053}, 'rouge-2': {'f': 0.1992393909684264, 'p': 0.19046146701483258, 'r': 0.22262230082962056}, 'rouge-l': {'f': 0.4024112886450365, 'p': 0.3852430050046152, 'r': 0.44030556796284664}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "literary-submission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tennessee judge lila statom has become a viral sensation thanks to her no - nonsense, brutal take down of a gang member who dared try to claim an area as'his hood'after o'shae smith told the judge he shot a rival gang member for being'in his hood,'she had no problem telling him the area didn't actually belong to him. smith has been charged with attempted first - degree murder for shooting kendre allen, 18, in the hip in late march.\n",
      "o'shae smith told judge lila statom that he shot a rival gang member for being'in his hood'last month. statom told him east court lakes, the housing project where the shooting took place, belonged only to the hard - working people who live there. the chattanooga area has been ravaged by gang violence. statom said she wanted to show smith that the courtroom was an area he couldn't intimidate or control.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "rand_id = random.randrange(len(hyps))\n",
    "print(hyps[rand_id])\n",
    "print(refs[rand_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
